{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python : 3.10.16\n",
    "# Created: Mar. 21. 2025\n",
    "# Updated: Mar. 21. 2025\n",
    "# Author: D.W. SHIN\n",
    "# Description: ë¯¸ìŠ¤íŠ¸ë„ 7B ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë¡œì»¬ PCì—ì„œ ê¸ˆìœµ ë°ì´í„° íŒŒì¸íŠœë‹í•˜ê¸°\n",
    "# Huggingfaceì™€ WandBë¥¼ ì´ìš©í•©ë‹ˆë‹¤.\n",
    "#\n",
    "# ì°¸ê³ ë¬¸ì„œ : ë„ë©”ì¸ íŠ¹í™” LLM: Mistral 7Bë¥¼ í™œìš©í•œ ê¸ˆìœµ ì—…ë¬´ë¶„ì•¼ íŒŒì¸íŠœë‹ ë° í™œìš© ë°©ë²• (ì •ì²œìˆ˜)\n",
    "# ë°ì´í„°ì…‹ ì¶œì²˜ : https://huggingface.co/datasets/csujeong/Non_life_insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \\\n",
    "# pip install transformers datasets accelerate peft bitsandbytes trl wandb huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ 2. ë°ì´í„° ë¡œë“œ (ë¡œì»¬ CSV íŒŒì¼ ì‚¬ìš©)\n",
    "DATA_PATH = \"./data/Non-life_insurance_Dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna()  # NaN ê°’ ì œê±°\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bb96b17d174bd9814002468e61b193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Œ 3. ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# ğŸ”¥ ë¡œê·¸ì¸í•œ Hugging Face í† í° ì‚¬ìš©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ 4. LoRA ì„¤ì • ë° ì ìš©\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_rank = 32\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdongweon-shin\u001b[0m (\u001b[33mdongweonshin\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shin/my_ws/shin-dev-backup-repo/fine_tune/wandb/run-20250321_102257-u0khbiw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9' target=\"_blank\">qlora_finetuning</a></strong> to <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance' target=\"_blank\">https://wandb.ai/dongweonshin/Mistral-7B-Finance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9' target=\"_blank\">https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73a618d49780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ“Œ 5. WandB ì„¤ì • (ë³´ì•ˆ ê°•í™” ë° `entity` ì¶”ê°€)\n",
    "wandb.login()  # ğŸ”¥ API Keyë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•˜ì§€ ì•Šê³  ë¡œê·¸ì¸ ìˆ˜í–‰\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Mistral-7B-Finance\",\n",
    "    name=\"qlora_finetuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ 6. í›ˆë ¨ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/Mistral-7B-Finetuned\",  # ë¡œì»¬ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=60,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e41a320775f457c8a89a23f3156c464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `formatting_func` ìˆ˜ì • (ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥¸ string í˜•ì‹ìœ¼ë¡œ ë³€í™˜)\n",
    "def formatting_func(example):\n",
    "    return {\"text\": str(example[\"QA_text\"]).strip()}  # ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ê³µë°± ì œê±°\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ ë³€í™˜í•˜ì—¬ ì˜¬ë°”ë¥´ê²Œ ì •ë¦¬\n",
    "dataset = dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7999626e98ba442facea4fdf918ff997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1addc25380de4e88a9b1f2541fd05e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa63a3254b14ccaa436850015418701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc0c5f8b90a48769afbac1a26aefdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ 7. Trainer ìƒì„±\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.348200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.4701295375823975, metrics={'train_runtime': 36.5596, 'train_samples_per_second': 6.565, 'train_steps_per_second': 1.641, 'total_flos': 1941095228866560.0, 'train_loss': 1.4701295375823975})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ“Œ 8. í•™ìŠµ ì‹¤í–‰\n",
    "peft_model.config.use_cache = False  # ìºì‹œ ë¹„í™œì„±í™”\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/Mistral-7B-Finetuned/tokenizer_config.json',\n",
       " './models/Mistral-7B-Finetuned/special_tokens_map.json',\n",
       " './models/Mistral-7B-Finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ“Œ 9. ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"./models/Mistral-7B-Finetuned\")\n",
    "tokenizer.save_pretrained(\"./models/Mistral-7B-Finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fdc13a09554307b499de98aa078ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Œ 10. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "PEFT_MODEL = \"./models/Mistral-7B-Finetuned\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ 11. í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸-ì‘ë‹µ ìƒì„± í•¨ìˆ˜)\n",
    "def generate_answer(prompt):\n",
    "    inputs = peft_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = peft_model.generate(**inputs, max_length=512)\n",
    "    return peft_tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³¨í”„ë³´í—˜ ì•Œë ¤ì¤˜? ê³¨í”„ë³´í—˜ì€ ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨í”„ë¥¼ í•˜ëŠ” ì¤‘ì— ë°œìƒí•˜ëŠ” ì‚¬ê³ ë¥¼ ë³´ìƒí•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤. ê³¨í”„ì¥ì—ì„œ ê³¨\n",
      "ì„ ë¬¼ì´ ë­ì•¼? ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ìœ¼ë¡œ ë¯¸ë˜ ê°€ì…ê³„ì•½ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ ë¬¼ì€ ë¯¸ë˜ ê°€ì…ê³„ì•½ì´ë¼ëŠ” ë§ ê·¸ëŒ€ë¡œ ë¯¸ë˜ì— ê°€ì…í•˜ëŠ” ê³„ì•½ì„ ë§í•©ë‹ˆë‹¤. ì„ \n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ 12. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(generate_answer(\"ê³¨í”„ë³´í—˜ ì•Œë ¤ì¤˜\"))\n",
    "print(generate_answer(\"ì„ ë¬¼ì´ ë­ì•¼?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„ (wandb)\n",
    "ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ê³¼ GPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ wandb ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ”¹ í•™ìŠµ ê³¼ì • (Training Metrics)\n",
    "ì•„ë˜ì˜ ê·¸ë˜í”„ëŠ” ëª¨ë¸ í›ˆë ¨ ì¤‘ ì†ì‹¤(loss), ì •í™•ë„(accuracy), í•™ìŠµë¥ (learning rate) ë“±ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "![Training Metrics](images/train_metrics.png)\n",
    "\n",
    "- **Loss ê°ì†Œ**: í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë˜ë©´ì„œ ëª¨ë¸ì´ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë¨ì„ ë³´ì—¬ì¤Œ.\n",
    "- **Mean Token Accuracy ì¦ê°€**: ëª¨ë¸ì´ ì ì  ë” ì •í™•í•œ ì¶œë ¥ì„ ìƒì„±í•˜ê³  ìˆìŒ.\n",
    "- **Learning Rate ê°ì†Œ**: ì•ˆì •ì ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ ë”°ë¥´ë©´ì„œ ìˆ˜ë ´ ì¤‘.\n",
    "\n",
    "## ğŸ”¹ GPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (System Monitoring)\n",
    "ì•„ë˜ì˜ ê·¸ë˜í”„ëŠ” í•™ìŠµ ì¤‘ GPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "![System Monitoring](images/system_monitoring.png)\n",
    "\n",
    "- **GPU ì‚¬ìš©ëŸ‰ ë³€ë™**: í•™ìŠµ ê³¼ì •ì—ì„œ GPU ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•¨.\n",
    "- **GPU í´ëŸ­ ì†ë„ ë³€í™”**: ëª¨ë¸ í•™ìŠµ ì¤‘ ì ì ˆí•œ ì—°ì‚° ì„±ëŠ¥ì„ ìœ ì§€.\n",
    "- **ì „ë ¥ ì œí•œ ì¤€ìˆ˜**: GPUì˜ ì „ë ¥ ì œí•œì´ ì´ˆê³¼ë˜ì§€ ì•Šìœ¼ë©°, ì‹œìŠ¤í…œ ì•ˆì •ì„±ì´ ìœ ì§€ë¨.\n",
    "\n",
    "ğŸ“Œ **ê²°ë¡ **: ëª¨ë¸ í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œìœ¼ë©°, GPU ë¦¬ì†ŒìŠ¤ê°€ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ íŠœë‹ì„ ì›í•  ê²½ìš°, `learning_rate`, `batch_size`, `gradient_accumulation_steps` ë“±ì„ ì¡°ì •í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
