{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python : 3.10.16\n",
    "# Created: Mar. 21. 2025\n",
    "# Updated: Mar. 21. 2025\n",
    "# Author: D.W. SHIN\n",
    "# Description: 미스트랄 7B 모델을 활용하여 로컬 PC에서 금융 데이터 파인튜닝하기\n",
    "# Huggingface와 WandB를 이용합니다.\n",
    "#\n",
    "# 참고문서 : 도메인 특화 LLM: Mistral 7B를 활용한 금융 업무분야 파인튜닝 및 활용 방법 (정천수)\n",
    "# 데이터셋 출처 : https://huggingface.co/datasets/csujeong/Non_life_insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 설치\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \\\n",
    "# pip install transformers datasets accelerate peft bitsandbytes trl wandb huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 1. 라이브러리 로드\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 2. 데이터 로드 (로컬 CSV 파일 사용)\n",
    "DATA_PATH = \"./data/Non-life_insurance_Dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna()  # NaN 값 제거\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bb96b17d174bd9814002468e61b193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📌 3. 모델 로드\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 🔥 로그인한 Hugging Face 토큰 사용\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 4. LoRA 설정 및 적용\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_rank = 32\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdongweon-shin\u001b[0m (\u001b[33mdongweonshin\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shin/my_ws/shin-dev-backup-repo/fine_tune/wandb/run-20250321_102257-u0khbiw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9' target=\"_blank\">qlora_finetuning</a></strong> to <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance' target=\"_blank\">https://wandb.ai/dongweonshin/Mistral-7B-Finance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9' target=\"_blank\">https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dongweonshin/Mistral-7B-Finance/runs/u0khbiw9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73a618d49780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📌 5. WandB 설정 (보안 강화 및 `entity` 추가)\n",
    "wandb.login()  # 🔥 API Key를 환경변수에 설정하지 않고 로그인 수행\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Mistral-7B-Finance\",\n",
    "    name=\"qlora_finetuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 6. 훈련 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/Mistral-7B-Finetuned\",  # 로컬 모델 저장 경로\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=60,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e41a320775f457c8a89a23f3156c464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# `formatting_func` 수정 (데이터를 올바른 string 형식으로 변환)\n",
    "def formatting_func(example):\n",
    "    return {\"text\": str(example[\"QA_text\"]).strip()}  # 문자열로 변환 후 공백 제거\n",
    "\n",
    "\n",
    "# 데이터셋을 변환하여 올바르게 정리\n",
    "dataset = dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7999626e98ba442facea4fdf918ff997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1addc25380de4e88a9b1f2541fd05e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa63a3254b14ccaa436850015418701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc0c5f8b90a48769afbac1a26aefdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# 📌 7. Trainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.348200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=1.4701295375823975, metrics={'train_runtime': 36.5596, 'train_samples_per_second': 6.565, 'train_steps_per_second': 1.641, 'total_flos': 1941095228866560.0, 'train_loss': 1.4701295375823975})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📌 8. 학습 실행\n",
    "peft_model.config.use_cache = False  # 캐시 비활성화\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/Mistral-7B-Finetuned/tokenizer_config.json',\n",
       " './models/Mistral-7B-Finetuned/special_tokens_map.json',\n",
       " './models/Mistral-7B-Finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📌 9. 모델 저장\n",
    "trainer.save_model(\"./models/Mistral-7B-Finetuned\")\n",
    "tokenizer.save_pretrained(\"./models/Mistral-7B-Finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fdc13a09554307b499de98aa078ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📌 10. 파인튜닝된 모델 불러오기\n",
    "PEFT_MODEL = \"./models/Mistral-7B-Finetuned\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 11. 테스트 (질문-응답 생성 함수)\n",
    "def generate_answer(prompt):\n",
    "    inputs = peft_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = peft_model.generate(**inputs, max_length=512)\n",
    "    return peft_tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "골프보험 알려줘? 골프보험은 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골프를 하는 중에 발생하는 사고를 보상하는 보험입니다. 골프장에서 골\n",
      "선물이 뭐야? 선물은 미래 가입계약으로 미래 가입계약은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선물은 미래 가입계약이라는 말 그대로 미래에 가입하는 계약을 말합니다. 선\n"
     ]
    }
   ],
   "source": [
    "# 📌 12. 테스트 실행\n",
    "print(generate_answer(\"골프보험 알려줘\"))\n",
    "print(generate_answer(\"선물이 뭐야?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 학습 결과 분석 (wandb)\n",
    "모델의 학습 과정과 GPU 성능 모니터링 데이터를 wandb 대시보드에서 확인할 수 있습니다.\n",
    "\n",
    "## 🔹 학습 과정 (Training Metrics)\n",
    "아래의 그래프는 모델 훈련 중 손실(loss), 정확도(accuracy), 학습률(learning rate) 등의 변화를 보여줍니다.\n",
    "\n",
    "![Training Metrics](images/train_metrics.png)\n",
    "\n",
    "- **Loss 감소**: 학습이 정상적으로 진행되면서 모델이 점진적으로 개선됨을 보여줌.\n",
    "- **Mean Token Accuracy 증가**: 모델이 점점 더 정확한 출력을 생성하고 있음.\n",
    "- **Learning Rate 감소**: 안정적인 학습률 스케줄을 따르면서 수렴 중.\n",
    "\n",
    "## 🔹 GPU 성능 모니터링 (System Monitoring)\n",
    "아래의 그래프는 학습 중 GPU 리소스 사용량을 나타냅니다.\n",
    "\n",
    "![System Monitoring](images/system_monitoring.png)\n",
    "\n",
    "- **GPU 사용량 변동**: 학습 과정에서 GPU 자원을 효율적으로 활용함.\n",
    "- **GPU 클럭 속도 변화**: 모델 학습 중 적절한 연산 성능을 유지.\n",
    "- **전력 제한 준수**: GPU의 전력 제한이 초과되지 않으며, 시스템 안정성이 유지됨.\n",
    "\n",
    "📌 **결론**: 모델 학습이 정상적으로 이루어졌으며, GPU 리소스가 효과적으로 활용됨을 확인할 수 있습니다. 추가적인 튜닝을 원할 경우, `learning_rate`, `batch_size`, `gradient_accumulation_steps` 등을 조정하여 실험을 진행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
