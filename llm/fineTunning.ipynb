{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "íŒŒì¸íŠœë‹ ì‹¤ìŠµì„ ì‹œì‘í•˜ê¸° ìœ„í•œ í™˜ê²½ì´ ê±°ì˜ ì¤€ë¹„ëœ ìƒíƒœì…ë‹ˆë‹¤. ë‹¤ìŒì€ í•„ìš”í•œ ì¶”ê°€ ì‘ì—… ë° ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë„êµ¬ ì„¤ì¹˜**\n",
    "íŒŒì¸íŠœë‹ì„ ìœ„í•´ ëª‡ ê°€ì§€ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. Hugging Faceì˜ `transformers`, `datasets`, `accelerate` íŒ¨í‚¤ì§€ê°€ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Miniconda í™˜ê²½ ìƒì„±\n",
    "```bash\n",
    "conda create -n llm-finetuning python=3.9 -y\n",
    "conda activate llm-finetuning\n",
    "```\n",
    "\n",
    "### íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install transformers datasets accelerate scipy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **ë°ì´í„° ì¤€ë¹„**\n",
    "íŒŒì¸íŠœë‹í•  ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. Hugging Faceì˜ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ê³µê°œ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆ: IMDB ê°ì„± ë¶„ì„ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "ë˜ëŠ”, ìì‹ ë§Œì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ CSV ë˜ëŠ” JSON í¬ë§·ìœ¼ë¡œ ì¤€ë¹„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ**\n",
    "Hugging Faceì˜ `transformers`ë¥¼ ì´ìš©í•´ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: BERT, GPT-2, T5)ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆ: BERT ëª¨ë¸ ë¡œë“œ\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **ë°ì´í„° ì „ì²˜ë¦¬**\n",
    "ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **íŒŒì¸íŠœë‹**\n",
    "Hugging Faceì˜ `Trainer` APIë¥¼ ì‚¬ìš©í•˜ë©´ íŒŒì¸íŠœë‹ì„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### TrainingArguments ë° Trainer ì„¤ì •\n",
    "```python\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "```\n",
    "\n",
    "### íŒŒì¸íŠœë‹ ì‹¤í–‰\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **ê²°ê³¼ ì €ì¥**\n",
    "íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "```python\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **ì¶”ë¡ **\n",
    "ìƒˆë¡œìš´ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "```python\n",
    "inputs = tokenizer(\"ì´ ì˜í™”ëŠ” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ì¶”ê°€ í•„ìš” ì‚¬í•­\n",
    "- **ë°ì´í„°ì…‹**: í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ê°€ ì—†ë‹¤ë©´ Hugging Face `datasets`ì—ì„œ ê³µê°œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ê±°ë‚˜ ì§ì ‘ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- **GPU ì‚¬ìš© í™•ì¸**: GPU í™œìš©ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "  ```bash\n",
    "  nvidia-smi\n",
    "  ```\n",
    "- **ë¬¸ì œ ì •ì˜**: ê°ì • ë¶„ì„, ë²ˆì—­, ìš”ì•½ ë“± ì‹¤ìŠµ ëª©ì ì— ë§ëŠ” ë¬¸ì œë¥¼ êµ¬ì²´í™”í•˜ì„¸ìš”.\n",
    "\n",
    "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆê±°ë‚˜ íŠ¹ì • ë¬¸ì œì— ëŒ€í•´ ë„ì›€ì„ ë°›ê³  ì‹¶ë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python : 3.9.21\n",
    "# Created: Jan. 03. 2025\n",
    "# Updated: Jan. 03. 2025\n",
    "# Author: D.W. SHIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf685a1009da41ed8fe39899a2fe8818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/miniconda3/envs/llm-finetuning/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_17403/599353726.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e598f0bff78548bc8d072fb529929fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3802, 'grad_norm': 19.89314842224121, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3016, 'grad_norm': 20.777158737182617, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2932, 'grad_norm': 11.884294509887695, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.268, 'grad_norm': 24.096893310546875, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2636, 'grad_norm': 8.69232177734375, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2595, 'grad_norm': 42.78128433227539, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2e3b604e7c43ea8eaa37bbf3944d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27073293924331665, 'eval_runtime': 91.7904, 'eval_samples_per_second': 272.36, 'eval_steps_per_second': 34.045, 'epoch': 1.0}\n",
      "{'loss': 0.1919, 'grad_norm': 0.05162442848086357, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1587, 'grad_norm': 0.5783259868621826, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1493, 'grad_norm': 1.3036930561065674, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1437, 'grad_norm': 0.04310528561472893, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.1689, 'grad_norm': 44.25569534301758, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n",
      "{'loss': 0.1414, 'grad_norm': 0.10130887478590012, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cc0b59d3b14ef98882301ca5a27bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27662393450737, 'eval_runtime': 91.8337, 'eval_samples_per_second': 272.231, 'eval_steps_per_second': 34.029, 'epoch': 2.0}\n",
      "{'loss': 0.1283, 'grad_norm': 0.09721854329109192, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n",
      "{'loss': 0.071, 'grad_norm': 0.024210352450609207, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n",
      "{'loss': 0.0623, 'grad_norm': 0.10222867876291275, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0743, 'grad_norm': 0.0219736248254776, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0793, 'grad_norm': 0.0652238056063652, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0722, 'grad_norm': 59.541385650634766, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aebd1bc0cda4e3690bddcebf7f3f1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3095468580722809, 'eval_runtime': 91.5513, 'eval_samples_per_second': 273.071, 'eval_steps_per_second': 34.134, 'epoch': 3.0}\n",
      "{'train_runtime': 1229.9229, 'train_samples_per_second': 60.979, 'train_steps_per_second': 7.622, 'train_loss': 0.1736964552815755, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.1736964552815755, metrics={'train_runtime': 1229.9229, 'train_samples_per_second': 60.979, 'train_steps_per_second': 7.622, 'total_flos': 1.9733329152e+16, 'train_loss': 0.1736964552815755, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_model/tokenizer_config.json',\n",
       " './finetuned_model/special_tokens_map.json',\n",
       " './finetuned_model/vocab.txt',\n",
       " './finetuned_model/added_tokens.json',\n",
       " './finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ëª¨ë¸ì„ GPU(CUDA)ë¡œ ì´ë™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°ë„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "inputs = tokenizer(\"This movie was fantastic!\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
