{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"1\",\"email\":\"user@example.com\",\"hashed_password\":\"$2b$12$xwRP2I46zwC4MYy4L0.ZiuTYn2ZQrmOn.EJMVg/eCRjJVXGdJWoYe\",\"is_active\":true,\"created_at\":\"2025-05-15T15:07:06.984041\",\"updated_at\":\"2025-05-15T15:07:06.984045\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"http://192.168.0.165:8100/api/v1/auth/register\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"email\": \"user@example.com\", \"password\": \"password123\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인증 토큰 발급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\",\"token_type\":\"bearer\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://192.168.0.165:8100/api/v1/auth/token \\\n",
    "  -d \"username=user@example.com&password=password123\" \\\n",
    "  -H \"Content-Type: application/x-www-form-urlencoded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF 목록 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pdfs\":[]}"
     ]
    }
   ],
   "source": [
    "!curl -X GET http://192.168.0.165:8100/pdf_list \\\n",
    "  -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":\"총 144개의 문서 청크가 벡터스토어에 저장되었습니다.\",\"filename\":\"sample.pdf\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://192.168.0.165:8100/upload \\\n",
    "  -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\" \\\n",
    "  -F \"file=@/Users/dongweonshin/Downloads/sample.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질의응답 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":\"<think>\\nOkay, the user is asking for a brief explanation of the Transformers model. Let me look at the provided documents to gather the key points.\\n\\nFirst, the documents mention that Transformers were introduced in 2017 by Google in the paper \\\"Attention is All You Need\\\" by Vaswani et al. The main idea is that they use only Attention Layers, which is different from previous models that relied on RNNs or CNNs. This architecture allows them to handle long-range dependencies better.\\n\\nThe documents also highlight that Transformers achieved SOTA (State Of The Art) in translation tasks. They're used in various fields like NLP and Computer Vision. The key components include the Masked Self-Attention, Feed-Forward Neural Networks, Layer Normalization, and Residual Connections. These components help in processing sequences, handling dependencies, and improving training stability.\\n\\nI should mention the origin, the use of attention mechanisms, the components, and the applications. Also, note that they're scalable and perform well with large data. Need to keep it concise, avoiding too much technical jargon but still accurate. Make sure to cite the sources properly as per the documents.\\n</think>\\n\\nTransformers는 2017년 구글에서 발표한 \\\"Attention is All You Need\\\" 논문에서 처음 소개된 딥러닝 모델 아키텍처입니다. 기존의 RNN이나 CNN에 의존했던 기존 모델과 달리, **어텐션(Attention) 메커니즘**만을 사용하여 시퀀스 데이터를 처리합니다. 이 모델은 번역 작업에서 최고의 성능(SOTA)을 달성했으며, 이후 자연어 처리(NLP), 컴퓨터 비전 등 다양한 분야에서 활용되고 있습니다.  \\n\\n주요 특징은 다음과 같습니다:  \\n1. **Masked Self-Attention**: 입력 시퀀스의 각 토큰이 자신 이전의 토큰만을 참조하여 어텐션을 계산합니다.  \\n2. **피드포워드 신경망**: 어텐션 출력에 비선형 변환을 적용해 복잡한 패턴을 학습합니다.  \\n3. **레이어 정규화(Layer Normalization)**와 **잔차 연결(Residual Connections)**: 학습 안정성과 훈련 속도를 향상시킵니다.  \\n\\n데이터량이 많을수록 성능이 향상되며, 확장성이 뛰어나 다양한 응용이 가능합니다. 출처: Vaswani et al., 2017.\\n\\n[참고 문서]\\n- sample.pdf (페이지: 14, 관련도: 높음)\\n- sample.pdf (페이지: 31, 관련도: 높음)\\n- sample.pdf (페이지: 48, 관련도: 높음)\\n- sample.pdf (페이지: 55, 관련도: 중간)\\n- sample.pdf (페이지: 55, 관련도: 중간)\",\"sources\":[{\"filename\":\"sample.pdf\",\"source\":\"/home/shin/my_ws/shin-dev-backup-repo/langgraph_rag_server/data/documents/1/sample.pdf\",\"page\":14,\"score\":0.5872156620025635,\"preview\":\"Transformers 모델 개요㎼ 구글의 㐮Attention is all you need ㏖Vaswani et al., 2017㏗㐯에 최초로 소개된 모델 architecture㎼\"},{\"filename\":\"sample.pdf\",\"source\":\"/home/shin/my_ws/shin-dev-backup-repo/langgraph_rag_server/data/documents/1/sample.pdf\",\"page\":31,\"score\":0.755780816078186,\"preview\":\"Transformer㎼ 2017년 구글에서 발표한 논문 및 모델 아키텍쳐㎼ Attention Layer 만을 활용하여 모델을 구성㎼ 번역 task에서 SOTA㏖State Of Th\"},{\"filename\":\"sample.pdf\",\"source\":\"/home/shin/my_ws/shin-dev-backup-repo/langgraph_rag_server/data/documents/1/sample.pdf\",\"page\":48,\"score\":0.8872528672218323,\"preview\":\"주요 컴포넌트 설명\"},{\"filename\":\"sample.pdf\",\"source\":\"/home/shin/my_ws/shin-dev-backup-repo/langgraph_rag_server/data/documents/1/sample.pdf\",\"page\":55,\"score\":0.9137450456619263,\"preview\":\"3. Transformer Decoder Layer1㏗ Masked Self㎼Attention:입력 시퀀스의 각 토큰이 자신 이전의 토큰들 만을 참조하여 어텐션을 수행, 쿼리㏖Qu\"},{\"filename\":\"sample.pdf\",\"source\":\"/home/shin/my_ws/shin-dev-backup-repo/langgraph_rag_server/data/documents/1/sample.pdf\",\"page\":55,\"score\":0.9143766164779663,\"preview\":\".substack.com/p/decoder㎼only㎼transformers㎼the㎼workhorse\"}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://192.168.0.165:8100/api/v1/rag/query \\\n",
    "  -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"question\": \"Transformers 모델에 대해 간단히 설명해 주세요.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":\"sample.pdf 삭제 완료\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST \"http://localhost:8100/delete_pdf?filename=sample.pdf\" \\\n",
    "  -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":\"sample.pdf 삭제 완료\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://192.168.0.165:8100/delete_pdf \\\n",
    "  -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJ1c2VyQGV4YW1wbGUuY29tIiwiZXhwIjoxNzQ3MjkxMDM1fQ.mt59ee-Hgkf6f4ybqI6AtzLuUTcinKR8UuNgj6meR9A\" \\\n",
    "  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n",
    "  -d \"filename=sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_rag_server_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
